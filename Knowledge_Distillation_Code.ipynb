{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["V0WQv5OvQ93z","bPAADmhc8KCv","lJxFXBZC8EAk","cxXC2ztPRD3f","A7W8stVi9d4P","_8qc-u31717v","WlPA1WpTN3y2","e4na_np-PJbT","ssmW2CBtO56f","1JPL0H0p-gqf"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"premium","accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"8f2250ef60164055aa599cdad68c4754":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_23e67c329d3c410d8099dfa24d5ea65a","IPY_MODEL_2e2a103c72cf4ee6bcd3c4eb52abef7c","IPY_MODEL_29984cae15d041d58200e99286231760"],"layout":"IPY_MODEL_770c478c90bc4f118ae3a1f39606fb1d"}},"23e67c329d3c410d8099dfa24d5ea65a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_642b500de8054b96953a6ebec40eb3e0","placeholder":"​","style":"IPY_MODEL_090393263c6b4496886bf959c0c82cfe","value":"100%"}},"2e2a103c72cf4ee6bcd3c4eb52abef7c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_911bffe17323428e9be4a99f8c69e795","max":169001437,"min":0,"orientation":"horizontal","style":"IPY_MODEL_970139808ab54359b012d8cdb8ef4e3f","value":169001437}},"29984cae15d041d58200e99286231760":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aefc49d7529e46198f551a86fb4875c4","placeholder":"​","style":"IPY_MODEL_25304cb4940b480c83468b27df896986","value":" 169001437/169001437 [00:13&lt;00:00, 14008897.28it/s]"}},"770c478c90bc4f118ae3a1f39606fb1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"642b500de8054b96953a6ebec40eb3e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"090393263c6b4496886bf959c0c82cfe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"911bffe17323428e9be4a99f8c69e795":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"970139808ab54359b012d8cdb8ef4e3f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aefc49d7529e46198f551a86fb4875c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25304cb4940b480c83468b27df896986":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"F2dh6sB1EqiA"},"outputs":[],"source":["# import necessary dependencies\n","from functools import partial\n","from typing import Any, Callable, List, Optional, Type, Union\n","\n","import torch\n","import torch.nn as nn\n","from torch import Tensor\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import argparse\n","import os, sys\n","import time\n","import datetime\n","from tqdm import tqdm_notebook as tqdm"]},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch.nn.functional as F"],"metadata":{"id":"yqbuj0-J3oMN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(\n","        in_planes,\n","        out_planes,\n","        kernel_size=3,\n","        stride=stride,\n","        padding=dilation,\n","        groups=groups,\n","        bias=False,\n","        dilation=dilation,\n","    )\n","\n","def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n","    \"\"\"1x1 convolution\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","class BasicBlock(nn.Module):\n","    expansion: int = 1\n","\n","    def __init__(\n","        self,\n","        inplanes: int,\n","        planes: int,\n","        stride: int = 1,\n","        downsample: Optional[nn.Module] = None,\n","        groups: int = 1,\n","        base_width: int = 64,\n","        dilation: int = 1,\n","        norm_layer: Optional[Callable[..., nn.Module]] = None,\n","    ) -> None:\n","        super().__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        if groups != 1 or base_width != 64:\n","            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n","        if dilation > 1:\n","            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.bn1 = norm_layer(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = norm_layer(planes)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n","\n","class Bottleneck(nn.Module):\n","    expansion: int = 4\n","\n","    def __init__(\n","        self,\n","        inplanes: int,\n","        planes: int,\n","        stride: int = 1,\n","        downsample: Optional[nn.Module] = None,\n","        groups: int = 1,\n","        base_width: int = 64,\n","        dilation: int = 1,\n","        norm_layer: Optional[Callable[..., nn.Module]] = None,\n","    ) -> None:\n","        super().__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        width = int(planes * (base_width / 64.0)) * groups\n","        self.conv1 = conv1x1(inplanes, width)\n","        self.bn1 = norm_layer(width)\n","        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n","        self.bn2 = norm_layer(width)\n","        self.conv3 = conv1x1(width, planes * self.expansion)\n","        self.bn3 = norm_layer(planes * self.expansion)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        identity = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out\n","\n","class ResNet(nn.Module):\n","    def __init__(\n","        self,\n","        block: Type[Union[BasicBlock, Bottleneck]],\n","        layers: List[int],\n","        num_classes: int = 10,\n","        zero_init_residual: bool = False,\n","        groups: int = 1,\n","        width_per_group: int = 64,\n","        replace_stride_with_dilation: Optional[List[bool]] = None,\n","        norm_layer: Optional[Callable[..., nn.Module]] = None,\n","    ) -> None:\n","        super().__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        self._norm_layer = norm_layer\n","\n","        self.inplanes = 64\n","        self.dilation = 1\n","        if replace_stride_with_dilation is None:\n","            # each element in the tuple indicates if we should replace\n","            # the 2x2 stride with a dilated convolution instead\n","            replace_stride_with_dilation = [False, False, False]\n","        if len(replace_stride_with_dilation) != 3:\n","            raise ValueError(\n","                \"replace_stride_with_dilation should be None \"\n","                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n","            )\n","        self.groups = groups\n","        self.base_width = width_per_group\n","        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = norm_layer(self.inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(512 * block.expansion, num_classes)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n","            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","\n","        # Zero-initialize the last BN in each residual branch,\n","        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n","        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n","        if zero_init_residual:\n","            for m in self.modules():\n","                if isinstance(m, Bottleneck) and m.bn3.weight is not None:\n","                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n","                elif isinstance(m, BasicBlock) and m.bn2.weight is not None:\n","                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n","\n","    def _make_layer(\n","        self,\n","        block: Type[Union[BasicBlock, Bottleneck]],\n","        planes: int,\n","        blocks: int,\n","        stride: int = 1,\n","        dilate: bool = False,\n","    ) -> nn.Sequential:\n","        norm_layer = self._norm_layer\n","        downsample = None\n","        previous_dilation = self.dilation\n","        if dilate:\n","            self.dilation *= stride\n","            stride = 1\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                conv1x1(self.inplanes, planes * block.expansion, stride),\n","                norm_layer(planes * block.expansion),\n","            )\n","\n","        layers = []\n","        layers.append(\n","            block(\n","                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n","            )\n","        )\n","        self.inplanes = planes * block.expansion\n","        for _ in range(1, blocks):\n","            layers.append(\n","                block(\n","                    self.inplanes,\n","                    planes,\n","                    groups=self.groups,\n","                    base_width=self.base_width,\n","                    dilation=self.dilation,\n","                    norm_layer=norm_layer,\n","                )\n","            )\n","\n","        return nn.Sequential(*layers)\n","\n","    def _forward_impl(self, x: Tensor) -> Tensor:\n","        # See note [TorchScript super()]\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        # x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc(x)\n","\n","        return x\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        return self._forward_impl(x)"],"metadata":{"id":"5TaFqkXPKoNP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# specify the device for computation\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","if device =='cuda':\n","    print(\"Run on GPU...\")\n","else:\n","    print(\"Run on CPU...\")\n","\n","ResNet18_model = ResNet(BasicBlock, [2, 2, 2, 2])\n","ResNet18_model = ResNet18_model.to(device)\n","\n","!nvidia-smi"],"metadata":{"id":"a8hhlbIhIUaw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"350bf8e1-f6d0-4420-e753-5f49df7af986"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Run on GPU...\n","Fri Nov 25 00:53:43 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0    49W / 400W |   1060MiB / 40536MiB |      5%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","source":["# CIFAR-10"],"metadata":{"id":"2angmzE0jXPN"}},{"cell_type":"code","source":["# Image preprocessing modules\n","transform_train = transforms.Compose([\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n","])\n","\n","transform_val = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n","])"],"metadata":{"id":"NkYbOuNII3v6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#from tools.dataset import CIFAR10\n","from torch.utils.data import DataLoader\n","\n","# a few arguments, do NOT change these\n","DATA_ROOT = \"./data\"\n","TRAIN_BATCH_SIZE = 256\n","VAL_BATCH_SIZE = 256\n","\n","\n","# CIFAR-10 dataset\n","train_dataset = torchvision.datasets.CIFAR10(root=DATA_ROOT,train=True,transform=transform_train,download=True)\n","\n","test_dataset = torchvision.datasets.CIFAR10(root=DATA_ROOT,train=False,transform=transform_val)\n","\n","# Data loader\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=TRAIN_BATCH_SIZE,shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=VAL_BATCH_SIZE,shuffle=False)"],"metadata":{"id":"kw-puL13Jqip"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# CIFAR-100"],"metadata":{"id":"mm_vnrrbjpbC"}},{"cell_type":"code","source":["# Image preprocessing modules\n","transform_train = transforms.Compose([\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n","])\n","\n","transform_val = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n","])"],"metadata":{"id":"U3-FpKOGjcXS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#from tools.dataset import CIFAR100\n","from torch.utils.data import DataLoader\n","\n","# a few arguments, do NOT change these\n","DATA_ROOT = \"./data\"\n","TRAIN_BATCH_SIZE = 256\n","VAL_BATCH_SIZE = 256\n","\n","# CIFAR-100 dataset\n","train_dataset = torchvision.datasets.CIFAR100(root=DATA_ROOT,train=True,transform=transform_train,download=True)\n","\n","test_dataset = torchvision.datasets.CIFAR100(root=DATA_ROOT,train=False,transform=transform_val)\n","\n","# Data loader\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=TRAIN_BATCH_SIZE,shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=VAL_BATCH_SIZE,shuffle=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":85,"referenced_widgets":["8f2250ef60164055aa599cdad68c4754","23e67c329d3c410d8099dfa24d5ea65a","2e2a103c72cf4ee6bcd3c4eb52abef7c","29984cae15d041d58200e99286231760","770c478c90bc4f118ae3a1f39606fb1d","642b500de8054b96953a6ebec40eb3e0","090393263c6b4496886bf959c0c82cfe","911bffe17323428e9be4a99f8c69e795","970139808ab54359b012d8cdb8ef4e3f","aefc49d7529e46198f551a86fb4875c4","25304cb4940b480c83468b27df896986"]},"id":"gOlO3yt7jd2K","outputId":"c86c774c-c8b4-499d-d726-4dc3ddeac920"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/169001437 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f2250ef60164055aa599cdad68c4754"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-100-python.tar.gz to ./data\n"]}]},{"cell_type":"markdown","source":["# ResNet18"],"metadata":{"id":"Hjl9vKGdh-e0"}},{"cell_type":"code","source":["def train_resnet_18(init_lr: float, momentum: float, regularization: float, decay_factor: float, total_epochs: int, change_lr_epochs: List[int]):\n","  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","  if device =='cuda':\n","      print(\"Run on GPU...\")\n","  else:\n","      print(\"Run on CPU...\")\n","\n","  ResNet18_model = ResNet(BasicBlock, [2, 2, 2, 2])\n","  ResNet18_model = ResNet18_model.to(device)\n","  INITIAL_LR = init_lr\n","  MOMENTUM = momentum\n","  REG = regularization\n","  optimizer = torch.optim.SGD(ResNet18_model.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4)\n","  criterion = nn.CrossEntropyLoss()\n","  # some hyperparameters\n","  # total number of training epochs\n","  Total_EPOCHS = total_epochs\n","  already_find_best_val_acc_count = 0\n","\n","  current_learning_rate = INITIAL_LR\n","  # the folder where the trained model is saved\n","  CHECKPOINT_FOLDER = \"./saved_model\"\n","\n","  best_val_acc = 0\n","  best_val_epoch = 0\n","\n","  epoch_history = []\n","  train_accuracy_history = []\n","  validation_accuracy_history = []\n","\n","  print(\"==> Training starts!\")\n","  print(\"=\"*50)\n","  for i in range(0, Total_EPOCHS):\n","      \n","      # handle the learning rate scheduler.\n","      if i in change_lr_epochs == 0:\n","          current_learning_rate = current_learning_rate * DECAY\n","          for param_group in optimizer.param_groups:\n","              param_group['lr'] = current_learning_rate\n","          print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n","      \n","      print(\"Epoch %d Current learning rate %f\" %(i, current_learning_rate))\n","      #######################\n","      # switch to train mode\n","      ResNet18_model.train()\n","\n","      epoch_history.append(i)\n","      # this help you compute the training accuracy\n","      total_examples = 0\n","      correct_examples = 0\n","      train_loss = 0 # track training loss if you want\n","    \n","      # Train the model for 1 epoch.\n","      for batch_idx, (inputs, targets) in enumerate(train_loader):\n","          ####################################\n","          # copy inputs to device\n","          inputs = inputs.to(device)\n","          targets = targets.to(device)\n","          # compute the output and loss\n","          outputs = ResNet18_model(inputs)\n","          loss = criterion(outputs, targets)\n","          train_loss += loss\n","          # zero the gradient\n","          optimizer.zero_grad()\n","          # backpropagation\n","          loss.backward()\n","          # apply gradient and update the weights\n","          optimizer.step()\n","          # count the number of correctly predicted samples in the current batch\n","          _, pre_out = torch.max(outputs.data, 1)\n","          correct_examples = correct_examples + torch.sum(pre_out == targets)\n","          total_examples = total_examples + targets.shape[0]\n","          ####################################\n","                  \n","      avg_loss = train_loss / len(train_loader)\n","      avg_acc = correct_examples / total_examples\n","      print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n","      train_accuracy_history.append(avg_acc.cpu())\n","      # Validate on the validation dataset\n","      #######################\n","      # switch to eval mode\n","      ResNet18_model.eval()\n","\n","      # this help you compute the validation accuracy\n","      total_examples = 0\n","      correct_examples = 0\n","      val_loss = 0 # again, track the validation loss if you want\n","\n","      # disable gradient during validation, which can save GPU memory\n","      with torch.no_grad():\n","          for batch_idx, (inputs, targets) in enumerate(test_loader):\n","              ####################################\n","              # copy inputs to device\n","              inputs = inputs.to(device)\n","              targets = targets.to(device)\n","              # compute the output and loss\n","              outputs = ResNet18_model(inputs)\n","              loss = criterion(outputs, targets)\n","              val_loss += loss\n","              # count the number of correctly predicted samples in the current batch\n","              _, pre_out = torch.max(outputs.data, 1)\n","              correct_examples = correct_examples + torch.sum(pre_out == targets)\n","              total_examples = total_examples + targets.shape[0]\n","              ####################################\n","\n","      avg_loss = val_loss / len(test_loader)\n","      avg_acc = correct_examples / total_examples\n","      print(\"Validation loss: %.4f, Validation accuracy: %.4f, Best Validation accuracy: %.4f best_val_epoch: %d\" % (avg_loss, avg_acc, best_val_acc, best_val_epoch))\n","      validation_accuracy_history.append(avg_acc.cpu())\n","      # save the model checkpoint\n","      if avg_acc > best_val_acc:\n","          best_val_acc = avg_acc\n","          best_val_epoch = i\n","          already_find_best_val_acc_count = 0\n","          if not os.path.exists(CHECKPOINT_FOLDER):\n","              os.makedirs(CHECKPOINT_FOLDER)\n","          print(\"Saving ...\")\n","          state = {'state_dict': ResNet18_model.state_dict(),\n","                  'epoch': i,\n","                  'lr': current_learning_rate}\n","          torch.save(state, os.path.join(CHECKPOINT_FOLDER, 'only_train_ResNet18.pth'))\n","      else:\n","        already_find_best_val_acc_count = already_find_best_val_acc_count + 1\n","          \n","      print('')\n","\n","  print(\"=\"*50)\n","  print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")\n","  print(\"best_val_epoch: \" + str(best_val_epoch))"],"metadata":{"id":"N5YN6y29g9dk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ResNet50"],"metadata":{"id":"V0WQv5OvQ93z"}},{"cell_type":"code","source":["def train_resnet_50(init_lr: float, momentum: float, regularization: float, decay_factor: float, total_epochs: int, change_lr_epochs: List[int]):\n","  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","  if device =='cuda':\n","      print(\"Run on GPU...\")\n","  else:\n","      print(\"Run on CPU...\")\n","\n","  ResNet50_model = ResNet(BasicBlock, [3, 4, 6, 3])\n","  ResNet50_model = ResNet50_model.to(device)\n","\n","  INITIAL_LR = init_lr\n","  MOMENTUM = momentum\n","  REG = regularization\n","  DECAY = decay_factor\n","  optimizer = optim.SGD(ResNet50_model.parameters(), lr=INITIAL_LR, momentum=MOMENTUM, weight_decay=REG)\n","  criterion = nn.CrossEntropyLoss()\n","\n","  # some hyperparameters\n","  # total number of training epochs\n","  Total_EPOCHS = total_epochs\n","  already_find_best_val_acc_count = 0\n","  current_learning_rate = INITIAL_LR\n","  # the folder where the trained model is saved\n","  CHECKPOINT_FOLDER = \"./saved_model\"\n","\n","  best_val_acc = 0\n","  best_val_epoch = 0\n","\n","  epoch_history = []\n","  train_accuracy_history = []\n","  validation_accuracy_history = []\n","\n","  print(\"==> Training starts!\")\n","  print(\"=\"*50)\n","  for i in range(0, Total_EPOCHS):\n","      \n","      # handle the learning rate scheduler.\n","      if i in change_lr_epochs == 0:\n","            current_learning_rate = current_learning_rate * DECAY\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] = current_learning_rate\n","            print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n","\n","      print(\"Epoch %d Current learning rate %f\" %(i, current_learning_rate))\n","      #######################\n","      # switch to train mode\n","      ResNet50_model.train()\n","      #######################\n","      epoch_history.append(i)\n","      # this help you compute the training accuracy\n","      total_examples = 0\n","      correct_examples = 0\n","      train_loss = 0 # track training loss if you want\n","      \n","      # Train the model for 1 epoch.\n","      for batch_idx, (inputs, targets) in enumerate(train_loader):\n","          ####################################\n","          # copy inputs to device\n","          inputs = inputs.to(device)\n","          targets = targets.to(device)\n","          # compute the output and loss\n","          outputs = ResNet50_model(inputs)\n","          loss = criterion(outputs, targets)\n","          train_loss += loss\n","          # zero the gradient\n","          optimizer.zero_grad()\n","          # backpropagation\n","          loss.backward()\n","          # apply gradient and update the weights\n","          optimizer.step()\n","          # count the number of correctly predicted samples in the current batch\n","          _, pre_out = torch.max(outputs.data, 1)\n","          correct_examples = correct_examples + torch.sum(pre_out == targets)\n","          total_examples = total_examples + targets.shape[0]\n","          ####################################\n","                  \n","      avg_loss = train_loss / len(train_loader)\n","      avg_acc = correct_examples / total_examples\n","      print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n","      train_accuracy_history.append(avg_acc.cpu())\n","      # Validate on the validation dataset\n","      #######################\n","      # switch to eval mode\n","      ResNet50_model.eval()\n","      #######################\n","\n","      # this help you compute the validation accuracy\n","      total_examples = 0\n","      correct_examples = 0\n","      \n","      val_loss = 0 # again, track the validation loss if you want\n","\n","      # disable gradient during validation, which can save GPU memory\n","      with torch.no_grad():\n","          for batch_idx, (inputs, targets) in enumerate(test_loader):\n","              ####################################\n","              # copy inputs to device\n","              inputs = inputs.to(device)\n","              targets = targets.to(device)\n","              # compute the output and loss\n","              outputs = ResNet50_model(inputs)\n","              loss = criterion(outputs, targets)\n","              val_loss += loss\n","              # count the number of correctly predicted samples in the current batch\n","              _, pre_out = torch.max(outputs.data, 1)\n","              correct_examples = correct_examples + torch.sum(pre_out == targets)\n","              total_examples = total_examples + targets.shape[0]\n","              ####################################\n","\n","      avg_loss = val_loss / len(test_loader)\n","      avg_acc = correct_examples / total_examples\n","      print(\"Validation loss: %.4f, Validation accuracy: %.4f, Best Validation accuracy: %.4f best_val_epoch: %d\" % (avg_loss, avg_acc, best_val_acc, best_val_epoch))\n","      validation_accuracy_history.append(avg_acc.cpu())\n","      # save the model checkpoint\n","      if avg_acc > best_val_acc:\n","          best_val_acc = avg_acc\n","          best_val_epoch = i\n","          already_find_best_val_acc_count = 0\n","          if not os.path.exists(CHECKPOINT_FOLDER):\n","              os.makedirs(CHECKPOINT_FOLDER)\n","          print(\"Saving ...\")\n","          state = {'state_dict': ResNet50_model.state_dict(),\n","                  'epoch': i,\n","                  'lr': current_learning_rate}\n","          torch.save(state, os.path.join(CHECKPOINT_FOLDER, 'only_train_ResNet50.pth'))\n","      else:\n","        already_find_best_val_acc_count = already_find_best_val_acc_count + 1\n","          \n","      print('')\n","\n","  print(\"=\"*50)\n","  print(f\"==> Optimization finished! ResNet50 Best validation accuracy: {best_val_acc:.4f}\")\n","  print(\"best_val_epoch: \" + str(best_val_epoch))"],"metadata":{"id":"aUX--OvRgApZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ResNet50 Teacher ResNet18 Student"],"metadata":{"id":"bPAADmhc8KCv"}},{"cell_type":"code","source":["def train_resnet18_student_resnet50_teacher(init_lr: float, momentum: float, regularization: float, decay: float, total_epochs: int, change_lr_epochs: List[int], temp: float, alpha: float):\n","\n","  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","  if device =='cuda':\n","      print(\"Run on GPU...\")\n","  else:\n","      print(\"Run on CPU...\")\n","\n","  Pretrained_ResNet50_model = ResNet(BasicBlock, [3, 4, 6, 3])\n","\n","  state_dict = torch.load('./saved_model/only_train_ResNet50.pth') # change the path to your own checkpoint file\n","  Pretrained_ResNet50_model.load_state_dict(state_dict['state_dict'])\n","  Pretrained_ResNet50_model = Pretrained_ResNet50_model.to(device)\n","\n","  ResNet18_model = ResNet(BasicBlock, [2, 2, 2, 2])\n","  ResNet18_model = ResNet18_model.to(device)\n","\n","  INITIAL_LR = init_lr\n","  MOMENTUM = momentum\n","  REG = regularization\n","  DECAY = decay\n","  optimizer = optim.SGD(ResNet18_model.parameters(), lr=INITIAL_LR, momentum=MOMENTUM, weight_decay=REG)\n","  criterion = nn.CrossEntropyLoss()\n","  DKL = nn.KLDivLoss()\n","\n","  # some hyperparameters\n","  # total number of training epochs\n","  Total_EPOCHS = total_epochs\n","  already_find_best_val_acc_count = 0\n","  current_learning_rate = INITIAL_LR\n","  # the folder where the trained model is saved\n","  CHECKPOINT_FOLDER = \"./saved_model\"\n","  temperature = temp\n","  alpha = alpha\n","\n","  best_val_acc = 0\n","  best_val_epoch = 0\n","\n","  epoch_history = []\n","  train_accuracy_history = []\n","  validation_accuracy_history = []\n","\n","  print(\"==> Training starts!\")\n","  print(\"=\"*50)\n","  for i in range(0, Total_EPOCHS):\n","      # handle the learning rate scheduler.\n","      if i in change_lr_epochs == 0:\n","            current_learning_rate = current_learning_rate * DECAY\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] = current_learning_rate\n","            print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n","    \n","      print(\"Epoch %d Current learning rate %f\" %(i, current_learning_rate))\n","      #######################\n","      # switch to train mode\n","      ResNet18_model.train()\n","      #######################\n","      epoch_history.append(i)\n","      # this help you compute the training accuracy\n","      total_examples = 0\n","      correct_examples = 0\n","      train_loss = 0 # track training loss if you want\n","      \n","      # Train the model for 1 epoch.\n","      for batch_idx, (inputs, targets) in enumerate(train_loader):\n","          ####################################\n","          # copy inputs to device\n","          inputs = inputs.to(device)\n","          targets = targets.to(device)\n","          # compute the output and loss\n","          outputs = ResNet18_model(inputs)\n","          Pretrained_output = Pretrained_ResNet50_model(inputs)\n","          \n","          soften_outputs = F.log_softmax(torch.div(outputs, temperature),dim =1)\n","          Pretrained_output = F.softmax(torch.div(Pretrained_output, temperature),dim =1)\n","          ###########################################################################################today remember to add alpha and 1-alpha on loss\n","          #print(\"criterion(outputs, targets):\", criterion(outputs, targets))\n","          #print(\"DKL(soften_outputs, Pretrained_output):\", DKL(soften_outputs, Pretrained_output))\n","          loss = (1-alpha) * criterion(outputs, targets) + alpha * DKL(soften_outputs, Pretrained_output)\n","          train_loss += loss\n","          # zero the gradient\n","          optimizer.zero_grad()\n","          # backpropagation\n","          loss.backward()\n","          # apply gradient and update the weights\n","          optimizer.step()\n","          # count the number of correctly predicted samples in the current batch\n","          _, pre_out = torch.max(outputs.data, 1)\n","          correct_examples = correct_examples + torch.sum(pre_out == targets)\n","          total_examples = total_examples + targets.shape[0]\n","          ####################################\n","                  \n","      avg_loss = train_loss / len(train_loader)\n","      avg_acc = correct_examples / total_examples\n","      print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n","      train_accuracy_history.append(avg_acc.cpu())\n","      # Validate on the validation dataset\n","      #######################\n","      # switch to eval mode\n","      ResNet18_model.eval()\n","      #######################\n","      # this help you compute the validation accuracy\n","      total_examples = 0\n","      correct_examples = 0\n","      val_loss = 0 # again, track the validation loss if you want\n","      # disable gradient during validation, which can save GPU memory\n","      with torch.no_grad():\n","          for batch_idx, (inputs, targets) in enumerate(test_loader):\n","              ####################################\n","              # your code here\n","              # copy inputs to device\n","              inputs = inputs.to(device)\n","              targets = targets.to(device)\n","              # compute the output and loss\n","              outputs = ResNet18_model(inputs)\n","              loss = criterion(outputs, targets)\n","              val_loss += loss\n","              # count the number of correctly predicted samples in the current batch\n","              _, pre_out = torch.max(outputs.data, 1)\n","              correct_examples = correct_examples + torch.sum(pre_out == targets)\n","              total_examples = total_examples + targets.shape[0]\n","              ####################################\n","\n","      avg_loss = val_loss / len(test_loader)\n","      avg_acc = correct_examples / total_examples\n","      print(\"Validation loss: %.4f, Validation accuracy: %.4f, Best Validation accuracy: %.4f best_val_epoch: %d\" % (avg_loss, avg_acc, best_val_acc, best_val_epoch))\n","      validation_accuracy_history.append(avg_acc.cpu())\n","      # save the model checkpoint\n","      if avg_acc > best_val_acc:\n","          best_val_acc = avg_acc\n","          best_val_epoch = i\n","          already_find_best_val_acc_count = 0\n","          if not os.path.exists(CHECKPOINT_FOLDER):\n","              os.makedirs(CHECKPOINT_FOLDER)\n","          print(\"Saving ...\")\n","          state = {'state_dict': ResNet18_model.state_dict(),\n","                  'epoch': i,\n","                  'lr': current_learning_rate}\n","          torch.save(state, os.path.join(CHECKPOINT_FOLDER, 'KD_ResNet50_Teacher_ResNet18_Student.pth'))\n","      else:\n","        already_find_best_val_acc_count = already_find_best_val_acc_count + 1\n","          \n","      print('')\n","\n","  print(\"=\"*50)\n","  print(f\"==> Optimization finished! ResNet 50 Teacher for ResNet18 Student Best validation accuracy: {best_val_acc:.4f}\")\n","  print(\"best_val_epoch: \" + str(best_val_epoch))"],"metadata":{"id":"Dc5DCQ8y8Mt4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ResNet18 Teacher ResNet50 Student"],"metadata":{"id":"lJxFXBZC8EAk"}},{"cell_type":"code","source":["def train_resnet50_student_resnet18_teacher(init_lr: float, momentum: float, regularization: float, decay: float, total_epochs: int, change_lr_epochs: List[int], temp: float, alpha: float):\n","  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","  if device =='cuda':\n","      print(\"Run on GPU...\")\n","  else:\n","      print(\"Run on CPU...\")\n","\n","  Pretrained_ResNet18_model = ResNet(BasicBlock, [2, 2, 2, 2])\n","\n","  state_dict = torch.load('./saved_model/only_train_ResNet18.pth') \n","  Pretrained_ResNet18_model.load_state_dict(state_dict['state_dict'])\n","  Pretrained_ResNet18_model = Pretrained_ResNet18_model.to(device)\n","\n","  ResNet50_model = ResNet(BasicBlock, [3, 4, 6, 3])\n","  ResNet50_model = ResNet50_model.to(device)\n","\n","  INITIAL_LR = init_lr\n","  MOMENTUM = momentum\n","  REG = regularization\n","  DECAY = decay\n","  optimizer = optim.SGD(ResNet50_model.parameters(), lr=INITIAL_LR, momentum=MOMENTUM, weight_decay=REG)\n","  criterion = nn.CrossEntropyLoss()\n","  DKL = nn.KLDivLoss()\n","\n","  # some hyperparameters\n","  # total number of training epochs\n","  Total_EPOCHS = total_epochs\n","  already_find_best_val_acc_count = 0\n","  current_learning_rate = INITIAL_LR\n","  # the folder where the trained model is saved\n","  CHECKPOINT_FOLDER = \"./saved_model\"\n","  temperature = temp\n","  alpha = alpha\n","  best_val_acc = 0\n","  best_val_epoch = 0\n","  epoch_history = []\n","  train_accuracy_history = []\n","  validation_accuracy_history = []\n","\n","  print(\"==> Training starts!\")\n","  print(\"=\"*50)\n","  for i in range(0, Total_EPOCHS):\n","      \n","      # handle the learning rate scheduler.\n","      if i in change_lr_epochs == 0:\n","            current_learning_rate = current_learning_rate * DECAY\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] = current_learning_rate\n","            print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n","    \n","      print(\"Epoch %d Current learning rate %f\" %(i, current_learning_rate))\n","      #######################\n","      # switch to train mode\n","      ResNet50_model.train()\n","      #######################\n","      epoch_history.append(i)\n","      # this help you compute the training accuracy\n","      total_examples = 0\n","      correct_examples = 0\n","      train_loss = 0 # track training loss if you want\n","      \n","      # Train the model for 1 epoch.\n","      for batch_idx, (inputs, targets) in enumerate(train_loader):\n","          ####################################\n","          # copy inputs to device\n","          inputs = inputs.to(device)\n","          targets = targets.to(device)\n","          # compute the output and loss\n","          outputs = ResNet50_model(inputs)\n","          Pretrained_output = Pretrained_ResNet18_model(inputs)\n","          \n","          soften_outputs = F.log_softmax(torch.div(outputs, temperature),dim =1)\n","          Pretrained_output = F.softmax(torch.div(Pretrained_output, temperature),dim =1)\n","          loss = (1-alpha) * criterion(outputs, targets) + alpha * DKL(soften_outputs, Pretrained_output)\n","          train_loss += loss\n","          # zero the gradient\n","          optimizer.zero_grad()\n","          # backpropagation\n","          loss.backward()\n","          # apply gradient and update the weights\n","          optimizer.step()\n","          # count the number of correctly predicted samples in the current batch\n","          _, pre_out = torch.max(outputs.data, 1)\n","          correct_examples = correct_examples + torch.sum(pre_out == targets)\n","          total_examples = total_examples + targets.shape[0]\n","          ####################################\n","                  \n","      avg_loss = train_loss / len(train_loader)\n","      avg_acc = correct_examples / total_examples\n","      print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n","      train_accuracy_history.append(avg_acc.cpu())\n","      # Validate on the validation dataset\n","      #######################\n","      # switch to eval mode\n","      ResNet50_model.eval()\n","      #######################\n","      # this help you compute the validation accuracy\n","      total_examples = 0\n","      correct_examples = 0\n","      val_loss = 0 # again, track the validation loss if you want\n","      # disable gradient during validation, which can save GPU memory\n","      with torch.no_grad():\n","          for batch_idx, (inputs, targets) in enumerate(test_loader):\n","              ####################################\n","              # your code here\n","              # copy inputs to device\n","              inputs = inputs.to(device)\n","              targets = targets.to(device)\n","              # compute the output and loss\n","              outputs = ResNet50_model(inputs)\n","              loss = criterion(outputs, targets)\n","              val_loss += loss\n","              # count the number of correctly predicted samples in the current batch\n","              _, pre_out = torch.max(outputs.data, 1)\n","              correct_examples = correct_examples + torch.sum(pre_out == targets)\n","              total_examples = total_examples + targets.shape[0]\n","              ####################################\n","\n","      avg_loss = val_loss / len(test_loader)\n","      avg_acc = correct_examples / total_examples\n","      print(\"Validation loss: %.4f, Validation accuracy: %.4f, Best Validation accuracy: %.4f best_val_epoch: %d\" % (avg_loss, avg_acc, best_val_acc, best_val_epoch))\n","      validation_accuracy_history.append(avg_acc.cpu())\n","      # save the model checkpoint\n","      if avg_acc > best_val_acc:\n","          best_val_acc = avg_acc\n","          best_val_epoch = i\n","          already_find_best_val_acc_count = 0\n","          if not os.path.exists(CHECKPOINT_FOLDER):\n","              os.makedirs(CHECKPOINT_FOLDER)\n","          print(\"Saving ...\")\n","          state = {'state_dict': ResNet18_model.state_dict(),\n","                  'epoch': i,\n","                  'lr': current_learning_rate}\n","          torch.save(state, os.path.join(CHECKPOINT_FOLDER, 'KD_ResNet18_Teacher_ResNet50_Student.pth'))\n","      else:\n","        already_find_best_val_acc_count = already_find_best_val_acc_count + 1\n","          \n","      print('')\n","\n","  print(\"=\"*50)\n","  print(f\"==> Optimization finished! ResNet18 Teacher for ResNet50 Student Best validation accuracy: {best_val_acc:.4f}\")\n","  print(\"best_val_epoch: \" + str(best_val_epoch))"],"metadata":{"id":"dPWg0FF-8Smt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Teacher-Free Self Knowledge Distillation"],"metadata":{"id":"cxXC2ztPRD3f"}},{"cell_type":"markdown","source":["Normal KD\n","ResNet_18 learns from Pretrained ResNet_18 Student: ResNet_18 Teacher: Self (Pretrained)"],"metadata":{"id":"S7mF-Q8AKC4j"}},{"cell_type":"code","source":["def train_resnet18_self_kd(init_lr: float, momentum: float, regularization: float, decay: float, total_epochs: int, change_lr_epochs: List[int], temp: float, alpha: float):\n","  # train_resnet_18(init_lr, momentum, regularization, decay, total_epochs, change_lr_epochs)\n","\n","  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","  if device =='cuda':\n","      print(\"Run on GPU...\")\n","  else:\n","      print(\"Run on CPU...\")\n","\n","  Pretrained_ResNet18_model = ResNet(BasicBlock, [2, 2, 2, 2])\n","  state_dict = torch.load('./saved_model/only_train_ResNet18.pth') # change the path to your own checkpoint file\n","  Pretrained_ResNet18_model.load_state_dict(state_dict['state_dict'])\n","  Pretrained_ResNet18_model = Pretrained_ResNet18_model.to(device)\n","\n","  ResNet18_model = ResNet(BasicBlock, [2, 2, 2, 2])\n","  ResNet18_model = ResNet18_model.to(device)\n","\n","  INITIAL_LR = init_lr\n","  MOMENTUM = momentum\n","  REG = regularization\n","  DECAY = decay\n","  optimizer = optim.SGD(ResNet18_model.parameters(), lr=INITIAL_LR, momentum=MOMENTUM, weight_decay=REG)\n","  criterion = nn.CrossEntropyLoss()\n","  DKL = nn.KLDivLoss()\n","\n","  # some hyperparameters\n","  # total number of training epochs\n","  Total_EPOCHS = total_epochs\n","  already_find_best_val_acc_count = 0\n","  current_learning_rate = INITIAL_LR\n","  # the folder where the trained model is saved\n","  CHECKPOINT_FOLDER = \"./saved_model\"\n","  temperature = temp\n","  alpha = alpha\n","  best_val_acc = 0\n","  best_val_epoch = 0\n","  epoch_history = []\n","  train_accuracy_history = []\n","  validation_accuracy_history = []\n","\n","  print(\"==> Training starts!\")\n","  print(\"=\"*50)\n","  for i in range(0, Total_EPOCHS):\n","      \n","      # handle the learning rate scheduler.\n","      if i in change_lr_epochs:\n","            current_learning_rate = current_learning_rate * DECAY\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] = current_learning_rate\n","            print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n","    \n","      print(\"Epoch %d Current learning rate %f\" %(i, current_learning_rate))\n","      #######################\n","      # switch to train mode\n","      ResNet18_model.train()\n","      #######################\n","\n","      epoch_history.append(i)\n","      # this help you compute the training accuracy\n","      total_examples = 0\n","      correct_examples = 0\n","      train_loss = 0 # track training loss if you want\n","      \n","      # Train the model for 1 epoch.\n","      for batch_idx, (inputs, targets) in enumerate(train_loader):\n","          ####################################\n","          # copy inputs to device\n","          inputs = inputs.to(device)\n","          targets = targets.to(device)\n","          # compute the output and loss\n","          outputs = ResNet18_model(inputs)\n","          Pretrained_output = Pretrained_ResNet18_model(inputs)\n","          soften_outputs = F.log_softmax(torch.div(outputs, temperature),dim =1)\n","          Pretrained_output = F.softmax(torch.div(Pretrained_output, temperature),dim =1)\n","          loss = (1-alpha) * criterion(outputs, targets) + alpha * DKL(soften_outputs, Pretrained_output)\n","          train_loss += loss\n","          # zero the gradient\n","          optimizer.zero_grad()\n","          # backpropagation\n","          loss.backward()\n","          # apply gradient and update the weights\n","          optimizer.step()\n","          # count the number of correctly predicted samples in the current batch\n","          _, pre_out = torch.max(outputs.data, 1)\n","          correct_examples = correct_examples + torch.sum(pre_out == targets)\n","          total_examples = total_examples + targets.shape[0]\n","          ####################################\n","                  \n","      avg_loss = train_loss / len(train_loader)\n","      avg_acc = correct_examples / total_examples\n","      print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n","      train_accuracy_history.append(avg_acc.cpu())\n","      # Validate on the validation dataset\n","      #######################\n","      # switch to eval mode\n","      ResNet18_model.eval()\n","      #######################\n","      # this help you compute the validation accuracy\n","      total_examples = 0\n","      correct_examples = 0\n","      val_loss = 0 # again, track the validation loss if you want\n","      # disable gradient during validation, which can save GPU memory\n","      with torch.no_grad():\n","          for batch_idx, (inputs, targets) in enumerate(test_loader):\n","              ####################################\n","              # your code here\n","              # copy inputs to device\n","              inputs = inputs.to(device)\n","              targets = targets.to(device)\n","              # compute the output and loss\n","              outputs = ResNet18_model(inputs)\n","              loss = criterion(outputs, targets)\n","              val_loss += loss\n","              # count the number of correctly predicted samples in the current batch\n","              _, pre_out = torch.max(outputs.data, 1)\n","              correct_examples = correct_examples + torch.sum(pre_out == targets)\n","              total_examples = total_examples + targets.shape[0]\n","              ####################################\n","\n","      avg_loss = val_loss / len(test_loader)\n","      avg_acc = correct_examples / total_examples\n","      print(\"Validation loss: %.4f, Validation accuracy: %.4f, Best Validation accuracy: %.4f best_val_epoch: %d\" % (avg_loss, avg_acc, best_val_acc, best_val_epoch))\n","      validation_accuracy_history.append(avg_acc.cpu())\n","      # save the model checkpoint\n","      if avg_acc > best_val_acc:\n","          best_val_acc = avg_acc\n","          best_val_epoch = i\n","          already_find_best_val_acc_count = 0\n","          if not os.path.exists(CHECKPOINT_FOLDER):\n","              os.makedirs(CHECKPOINT_FOLDER)\n","          print(\"Saving ...\")\n","          state = {'state_dict': ResNet18_model.state_dict(),\n","                  'epoch': i,\n","                  'lr': current_learning_rate}\n","          torch.save(state, os.path.join(CHECKPOINT_FOLDER, 'Tf_KD_self_ResNet18.pth'))\n","      else:\n","        already_find_best_val_acc_count = already_find_best_val_acc_count + 1\n","      print('')\n","\n","  print(\"=\"*50)\n","  print(f\"==> Optimization finished! ResNet18 Self Knowledge Distillation Best validation accuracy: {best_val_acc:.4f}\")\n","  print(\"best_val_epoch: \" + str(best_val_epoch))"],"metadata":{"id":"e998OZDcKVwx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Teacher-Free Knowledge Distillation Manually-Designated Regularization"],"metadata":{"id":"A7W8stVi9d4P"}},{"cell_type":"markdown","source":["We design a virtual teacher model based on targets, assigning weight a to the target and weight (1 - a) / (num_classes - 1) to all other classes"],"metadata":{"id":"AuiqvfslMT3m"}},{"cell_type":"code","source":["def train_resnet18_reg_kd(init_lr: float, momentum: float, regularization: float, decay: float, total_epochs: int, change_lr_epochs: List[int], temp: float, alpha: float, a_val: float, num_classes: int):\n","  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","  if device =='cuda':\n","      print(\"Run on GPU...\")\n","  else:\n","      print(\"Run on CPU...\")\n","\n","  ResNet18_model = ResNet(BasicBlock, [2, 2, 2, 2])\n","  ResNet18_model = ResNet18_model.to(device)\n","\n","  INITIAL_LR = init_lr\n","  MOMENTUM = momentum\n","  REG = regularization\n","  DECAY = decay\n","  optimizer = optim.SGD(ResNet18_model.parameters(), lr=INITIAL_LR, momentum=MOMENTUM, weight_decay=REG)\n","  criterion = nn.CrossEntropyLoss()\n","  DKL = nn.KLDivLoss()\n","\n","  # some hyperparameters\n","  # total number of training epochs\n","  Total_EPOCHS = total_epochs\n","  already_find_best_val_acc_count = 0\n","  current_learning_rate = INITIAL_LR\n","  # the folder where the trained model is saved\n","  CHECKPOINT_FOLDER = \"./saved_model\"\n","\n","  temperature = temp\n","  alpha = alpha\n","  a = a_val\n","  n_classes = num_classes\n","\n","  # start the training/validation process\n","  # the process should take about 5 minutes on a GTX 1070-Ti\n","  # if the code is written efficiently.\n","  best_val_acc = 0\n","  best_val_epoch = 0\n","\n","  epoch_history = []\n","  train_accuracy_history = []\n","  validation_accuracy_history = []\n","\n","  print(\"==> Training starts!\")\n","  print(\"=\"*50)\n","  for i in range(0, Total_EPOCHS):\n","      # handle the learning rate scheduler.\n","      if i in change_lr_epochs:\n","            current_learning_rate = current_learning_rate * DECAY\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] = current_learning_rate\n","            print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n","      \n","      print(\"Epoch %d Current learning rate %f\" %(i, current_learning_rate))\n","      #######################\n","      # switch to train mode\n","      ResNet18_model.train()\n","      #######################\n","\n","      epoch_history.append(i)\n","      # this help you compute the training accuracy\n","      total_examples = 0\n","      correct_examples = 0\n","      train_loss = 0 # track training loss if you want\n","      # Train the model for 1 epoch.\n","      for batch_idx, (inputs, targets) in enumerate(train_loader):\n","          ####################################\n","          # your code here\n","          # copy inputs to device\n","          inputs = inputs.to(device)\n","          targets = targets.to(device)\n","          # compute the output and loss\n","          outputs = ResNet18_model(inputs)\n","          detached_targets = targets.detach()\n","          Manual_output = torch.full((detached_targets.size()[0], n_classes), (1 - a) / (n_classes - 1))\n","          for i in range(detached_targets.size()[0]):\n","            Manual_output[i][detached_targets[i]] = a\n","          Manual_output = Manual_output.to(device)\n","          soften_outputs = F.log_softmax(torch.div(outputs, temperature),dim =1)\n","          Manual_output = F.softmax(torch.div(Manual_output, temperature),dim =1)\n","          loss = (1-alpha) * criterion(outputs, targets) + alpha * DKL(soften_outputs, Manual_output)\n","          train_loss += loss\n","          # zero the gradient\n","          optimizer.zero_grad()\n","          # backpropagation\n","          loss.backward()\n","          # apply gradient and update the weights\n","          optimizer.step()\n","          # count the number of correctly predicted samples in the current batch\n","          _, pre_out = torch.max(outputs.data, 1)\n","          correct_examples = correct_examples + torch.sum(pre_out == targets)\n","          total_examples = total_examples + targets.shape[0]\n","          ####################################\n","                  \n","      avg_loss = train_loss / len(train_loader)\n","      avg_acc = correct_examples / total_examples\n","      print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n","      train_accuracy_history.append(avg_acc.cpu())\n","      # Validate on the validation dataset\n","      #######################\n","      # switch to eval mode\n","      ResNet18_model.eval()\n","      #######################\n","\n","      # this help you compute the validation accuracy\n","      total_examples = 0\n","      correct_examples = 0\n","      val_loss = 0 # again, track the validation loss if you want\n","      # disable gradient during validation, which can save GPU memory\n","      with torch.no_grad():\n","          for batch_idx, (inputs, targets) in enumerate(test_loader):\n","              ####################################\n","              # copy inputs to device\n","              inputs = inputs.to(device)\n","              targets = targets.to(device)\n","              # compute the output and loss\n","              outputs = ResNet18_model(inputs)\n","              loss = criterion(outputs, targets)\n","              val_loss += loss\n","              # count the number of correctly predicted samples in the current batch\n","              _, pre_out = torch.max(outputs.data, 1)\n","              correct_examples = correct_examples + torch.sum(pre_out == targets)\n","              total_examples = total_examples + targets.shape[0]\n","              ####################################\n","\n","      avg_loss = val_loss / len(test_loader)\n","      avg_acc = correct_examples / total_examples\n","      print(\"Validation loss: %.4f, Validation accuracy: %.4f, Best Validation accuracy: %.4f best_val_epoch: %d\" % (avg_loss, avg_acc, best_val_acc, best_val_epoch))\n","      validation_accuracy_history.append(avg_acc.cpu())\n","      # save the model checkpoint\n","      if avg_acc > best_val_acc:\n","          best_val_acc = avg_acc\n","          best_val_epoch = i\n","          already_find_best_val_acc_count = 0\n","          if not os.path.exists(CHECKPOINT_FOLDER):\n","              os.makedirs(CHECKPOINT_FOLDER)\n","          print(\"Saving ...\")\n","          state = {'state_dict': ResNet18_model.state_dict(),\n","                  'epoch': i,\n","                  'lr': current_learning_rate}\n","          torch.save(state, os.path.join(CHECKPOINT_FOLDER, 'Tf_KD_reg_ResNet18.pth'))\n","      else:\n","        already_find_best_val_acc_count = already_find_best_val_acc_count + 1\n","          \n","      print('')\n","\n","  print(\"=\"*50)\n","  print(f\"==> Optimization finished! Teacher Free Knowledge Distillation ResNet18 Best validation accuracy: {best_val_acc:.4f}\")\n","  print(\"best_val_epoch: \" + str(best_val_epoch))"],"metadata":{"id":"LjmxVfJU6YPh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Adverserial"],"metadata":{"id":"_8qc-u31717v"}},{"cell_type":"code","source":["import attacks\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","import matplotlib.pyplot as plt\n","import random"],"metadata":{"id":"Vg-OffWS8leB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_model(mdl, loader, device):\n","    mdl.eval()\n","    running_correct = 0.\n","    running_loss = 0.\n","    running_total = 0.\n","    with torch.no_grad():\n","        for batch_idx,(data,labels) in enumerate(loader):\n","            data = data.to(device); labels = labels.to(device)\n","            clean_outputs = mdl(data)\n","            clean_loss = F.cross_entropy(clean_outputs, labels)\n","            _,clean_preds = clean_outputs.max(1)\n","            running_correct += clean_preds.eq(labels).sum().item()\n","            running_loss += clean_loss.item()\n","            running_total += labels.size(0)\n","    clean_acc = running_correct/running_total\n","    clean_loss = running_loss/len(loader)\n","    mdl.train()\n","    return clean_acc,clean_loss"],"metadata":{"id":"PMZCuw3c9LHG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_val, min_val = float('-inf'), float('inf')\n","for batch_idx,(data,labels) in enumerate(test_loader):\n","  max_val = max(torch.max(data).item(), max_val)\n","  min_val = min(torch.min(data).item(), min_val)\n","print('Max: ', max_val)\n","print('Min: ', min_val)"],"metadata":{"id":"4qkTW2hHOAtn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9a19feba-cfbb-4546-bad8-81ed41e96898"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Max:  2.7537312507629395\n","Min:  -2.429065704345703\n"]}]},{"cell_type":"markdown","source":["# ResNet18 Adversarial"],"metadata":{"id":"WlPA1WpTN3y2"}},{"cell_type":"code","source":["def train_resnet_18_adversarial(init_lr: float, momentum: float, regularization: float, decay_factor: float, total_epochs: int, change_lr_epochs: List[int], optim_type: str, eps: float, atk_iters: int, temp: float, alpha: float):\n","  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","  if device =='cuda':\n","      print(\"Run on GPU...\")\n","  else:\n","      print(\"Run on CPU...\")\n","  net = ResNet(BasicBlock, [2, 2, 2, 2])\n","  net = net.to(device)\n","  INITIAL_LR = init_lr\n","  MOMENTUM = momentum\n","  REG = regularization\n","  DECAY = decay_factor\n","  criterion = nn.CrossEntropyLoss()\n","  DKL = nn.KLDivLoss()\n","  Total_EPOCHS = total_epochs\n","  ATK_EPS = eps\n","  ATK_ITERS = atk_iters\n","  ATK_ALPHA = 1.85 * ATK_EPS / ATK_ITERS\n","  RND_START = True\n","  temperature = temp\n","  alpha = alpha\n","  if optim_type.lower() == 'sgd':\n","    optimizer = optim.SGD(net.parameters(), lr=INITIAL_LR, momentum=MOMENTUM, weight_decay=REG)\n","  else:\n","    optimizer = optim.Adam(net.parameters(), lr=INITIAL_LR, weight_decay=REG)\n","  criterion = nn.CrossEntropyLoss()\n","  current_learning_rate = INITIAL_LR\n","  # the folder where the trained model is saved\n","  CHECKPOINT_FOLDER = \"./saved_model\"\n","  already_find_best_val_acc_count = 0\n","  best_val_acc = 0\n","  best_val_epoch = 0\n","  epoch_history = []\n","  train_accuracy_history = []\n","  validation_accuracy_history = []\n","\n","  print(\"==> Training starts!\")\n","  print(\"=\"*50)\n","  for i in range(0, Total_EPOCHS):\n","      # handle the learning rate scheduler.\n","      if i in change_lr_epochs:\n","            current_learning_rate = current_learning_rate * DECAY\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] = current_learning_rate\n","            print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n","      \n","      print(\"Epoch %d Current learning rate %f\" %(i, current_learning_rate))\n","      #######################\n","      # switch to train mode\n","      net.train()\n","      epoch_history.append(i)\n","      # this help you compute the training accuracy\n","      total_examples = 0\n","      correct_examples = 0\n","      train_loss = 0 # track training loss if you want\n","      for batch_idx,(data,labels) in enumerate(train_loader):\n","        data = data.to(device); labels = labels.to(device)\n","        adv_data = attacks.PGD_attack(net, device, data, labels, ATK_EPS, ATK_ALPHA, ATK_ITERS, RND_START, min_val, max_val)\n","        \n","        # Forward pass\n","        outputs = net(adv_data)\n","        net.zero_grad()\n","        optimizer.zero_grad()\n","        # Compute loss, gradients, and update params\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        # Update stats\n","        _,preds = outputs.max(1)\n","        correct_examples += preds.eq(labels).sum().item()\n","        train_loss += loss.item()\n","        total_examples += labels.size(0)\n","        ####################################\n","                  \n","      avg_loss = train_loss / len(train_loader)\n","      avg_acc = correct_examples / total_examples\n","      train_accuracy_history.append(avg_acc)\n","      print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n","\n","      # switch to eval mode\n","      net.eval()\n","      # this help you compute the validation accuracy\n","      total_examples = 0\n","      correct_examples = 0\n","      \n","      val_loss = 0 # again, track the validation loss if you want\n","      with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(test_loader):\n","            ####################################\n","            inputs = inputs.to(device)\n","            targets = targets.to(device)\n","            # compute the output and loss\n","            outputs = net(inputs)\n","            # outputs = net(adv_data)\n","            loss = criterion(outputs, targets)\n","            val_loss += loss\n","            # count the number of correctly predicted samples in the current batch\n","            _, pre_out = torch.max(outputs.data, 1)\n","            correct_examples = correct_examples + torch.sum(pre_out == targets)\n","            total_examples = total_examples + targets.shape[0]\n","            ####################################\n","\n","      avg_loss = val_loss / len(test_loader)\n","      avg_acc = correct_examples / total_examples\n","      validation_accuracy_history.append(avg_acc)\n","      print(\"Validation loss: %.4f, Validation accuracy: %.4f, Best Validation accuracy: %.4f best_val_epoch: %d\" % (avg_loss, avg_acc, best_val_acc, best_val_epoch))\n","      # save the model checkpoint\n","      model_checkpoint = \"Adv_only_train_ResNet18.pth\"\n","      if avg_acc > best_val_acc:\n","          best_val_acc = avg_acc\n","          best_val_epoch = i\n","          already_find_best_val_acc_count = 0\n","          if not os.path.exists(CHECKPOINT_FOLDER):\n","              os.makedirs(CHECKPOINT_FOLDER)\n","          print(\"Saving ...\")\n","          state = {'state_dict': net.state_dict(),\n","                  'epoch': i,\n","                  'lr': current_learning_rate}\n","          torch.save(state, os.path.join(CHECKPOINT_FOLDER, model_checkpoint))\n","      else:\n","          already_find_best_val_acc_count = already_find_best_val_acc_count + 1\n","          \n","      print('')\n","\n","  print(\"=\"*50)\n","  print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")\n","  print(\"best_val_epoch: \" + str(best_val_epoch))"],"metadata":{"id":"wzJ0jPUq_rLJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Teacher-Free Self Knowledge Distillation Adversarial"],"metadata":{"id":"e4na_np-PJbT"}},{"cell_type":"code","source":["def train_resnet_18_self_kd_adversarial(init_lr: float, momentum: float, regularization: float, decay_factor: float, total_epochs: int, change_lr_epochs: List[int], optim_type: str, eps: float, atk_iters: int, temp: float, alpha: float):\n","  # train_resnet_18_adversarial(init_lr, momentum, regularization, decay_factor, total_epochs, change_lr_epochs, optim_type, eps, atk_iters, temp, alpha)\n","  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","  if device =='cuda':\n","      print(\"Run on GPU...\")\n","  else:\n","      print(\"Run on CPU...\")\n","\n","  net = ResNet(BasicBlock, [2, 2, 2, 2])\n","  net = net.to(device)\n","  Pretrained_ResNet18_model = ResNet(BasicBlock, [2, 2, 2, 2])\n","  state_dict = torch.load('./saved_model/Adv_only_train_ResNet18.pth') \n","  Pretrained_ResNet18_model.load_state_dict(state_dict['state_dict'])\n","  Pretrained_ResNet18_model = Pretrained_ResNet18_model.to(device)\n","\n","  INITIAL_LR = init_lr\n","  MOMENTUM = momentum\n","  REG = regularization\n","  DECAY = decay_factor\n","  criterion = nn.CrossEntropyLoss()\n","  DKL = nn.KLDivLoss()\n","\n","  if optim_type.lower() == 'sgd':\n","    optimizer = optim.SGD(net.parameters(), lr=INITIAL_LR, momentum=MOMENTUM, weight_decay=REG)\n","  else:\n","    optimizer = optim.Adam(net.parameters(), lr=INITIAL_LR, weight_decay=REG)\n","  \n","  ATK_EPS = eps\n","  ATK_ITERS = atk_iters\n","  ATK_ALPHA = 1.85 * ATK_EPS / ATK_ITERS\n","  RND_START = True\n","  temperature = temp\n","  alpha = alpha\n","\n","  criterion = nn.CrossEntropyLoss()\n","  # some hyperparameters\n","  # total number of training epochs\n","  Total_EPOCHS = total_epochs\n","  already_find_best_val_acc_count = 0\n","\n","  current_learning_rate = INITIAL_LR\n","  # the folder where the trained model is saved\n","  CHECKPOINT_FOLDER = \"./saved_model\"\n","\n","  # start the training/validation process\n","  # the process should take about 5 minutes on a GTX 1070-Ti\n","  # if the code is written efficiently.\n","  best_val_acc = 0\n","  best_val_epoch = 0\n","\n","  epoch_history = []\n","  train_accuracy_history = []\n","  validation_accuracy_history = []\n","\n","  print(\"==> Training starts!\")\n","  print(\"=\"*50)\n","  for i in range(0, Total_EPOCHS):\n","      if i in change_lr_epochs:\n","            current_learning_rate = current_learning_rate * DECAY\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] = current_learning_rate\n","            print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n","      print(\"Epoch %d Current learning rate %f\" %(i, current_learning_rate))\n","\n","      net.train()\n","      \n","      #######################\n","      epoch_history.append(i)\n","      # this help you compute the training accuracy\n","      total_examples = 0\n","      correct_examples = 0\n","      train_loss = 0 # track training loss if you want\n","      # Train the model for 1 epoch.\n","      for batch_idx,(data,labels) in enumerate(train_loader):\n","          data = data.to(device); labels = labels.to(device)\n","          \n","          adv_data = attacks.PGD_attack(net, device, data, labels, ATK_EPS, ATK_ALPHA, ATK_ITERS, RND_START, min_val, max_val)\n","          # Forward pass\n","          outputs = net(adv_data)\n","          net.zero_grad()\n","          Pretrained_output = Pretrained_ResNet18_model(data)\n","          soften_outputs = F.log_softmax(torch.div(outputs, temperature),dim =1)\n","          Pretrained_output = F.softmax(torch.div(Pretrained_output, temperature),dim =1)\n","          # zero the gradient\n","          optimizer.zero_grad()\n","          # Compute loss, gradients, and update params\n","          loss = (1-alpha) * criterion(outputs, labels) + alpha * DKL(soften_outputs, Pretrained_output)\n","          loss.backward()\n","          optimizer.step()\n","          # Update stats\n","          _,preds = outputs.max(1)\n","          correct_examples += preds.eq(labels).sum().item()\n","          train_loss += loss.item()\n","          total_examples += labels.size(0)\n","          ####################################\n","                  \n","      avg_loss = train_loss / len(train_loader)\n","      avg_acc = correct_examples / total_examples\n","      train_accuracy_history.append(avg_acc)\n","      print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n","\n","      # switch to eval mode\n","      net.eval()\n","      # this help you compute the validation accuracy\n","      total_examples = 0\n","      correct_examples = 0\n","      val_loss = 0 # again, track the validation loss if you want\n","      with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(test_loader):\n","            ####################################\n","            inputs = inputs.to(device)\n","            targets = targets.to(device)\n","            # compute the output and loss\n","            outputs = net(inputs)\n","            # outputs = net(adv_data)\n","            loss = criterion(outputs, targets)\n","            val_loss += loss\n","            # count the number of correctly predicted samples in the current batch\n","            _, pre_out = torch.max(outputs.data, 1)\n","            correct_examples = correct_examples + torch.sum(pre_out == targets)\n","            total_examples = total_examples + targets.shape[0]\n","            ####################################\n","\n","      avg_loss = val_loss / len(test_loader)\n","      avg_acc = correct_examples / total_examples\n","      validation_accuracy_history.append(avg_acc)\n","      print(\"Validation loss: %.4f, Validation accuracy: %.4f, Best Validation accuracy: %.4f best_val_epoch: %d\" % (avg_loss, avg_acc, best_val_acc, best_val_epoch))\n","      # save the model checkpoint\n","      model_checkpoint = \"Adv_Self_KD_ResNet18.pth\"\n","      if avg_acc > best_val_acc:\n","          best_val_acc = avg_acc\n","          best_val_epoch = i\n","          already_find_best_val_acc_count = 0\n","          if not os.path.exists(CHECKPOINT_FOLDER):\n","              os.makedirs(CHECKPOINT_FOLDER)\n","          print(\"Saving ...\")\n","          state = {'state_dict': net.state_dict(),\n","                  'epoch': i,\n","                  'lr': current_learning_rate}\n","          torch.save(state, os.path.join(CHECKPOINT_FOLDER, model_checkpoint))\n","      else:\n","        already_find_best_val_acc_count = already_find_best_val_acc_count + 1\n","          \n","      print('')\n","\n","  print(\"=\"*50)\n","  print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")\n","  print(\"best_val_epoch: \" + str(best_val_epoch))\n"],"metadata":{"id":"7V6vjOn3f0Y-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Teacher-Free Manually-Designated Regularization Adversarial"],"metadata":{"id":"ssmW2CBtO56f"}},{"cell_type":"code","source":["def train_resnet_18_reg_kd_adversarial(init_lr: float, momentum: float, regularization: float, decay_factor: float, total_epochs: int, change_lr_epochs: List[int], optim_type: str, eps: float, atk_iters: int, temp: float, alpha: float, a_val: float, num_classes: int):\n","  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","  if device =='cuda':\n","      print(\"Run on GPU...\")\n","  else:\n","      print(\"Run on CPU...\")\n","\n","  net = ResNet(BasicBlock, [2, 2, 2, 2])\n","  net = net.to(device)\n","\n","  INITIAL_LR = init_lr\n","  MOMENTUM = momentum\n","  REG = regularization\n","  DECAY = decay_factor\n","\n","  if optim_type.lower() == 'sgd':\n","    optimizer = optim.SGD(net.parameters(), lr=INITIAL_LR, momentum=MOMENTUM, weight_decay=REG)\n","  else:\n","    optimizer = optim.Adam(net.parameters(), lr=INITIAL_LR, weight_decay=REG)\n","\n","  ATK_EPS = eps\n","  ATK_ITERS = atk_iters\n","  ATK_ALPHA = 1.85 * ATK_EPS / ATK_ITERS\n","  RND_START = True\n","  n_classes = num_classes\n","  a = a_val\n","  temperature = temp\n","  alpha = alpha\n","\n","  criterion = nn.CrossEntropyLoss()\n","  DKL = nn.KLDivLoss()\n","  # some hyperparameters\n","  # total number of training epochs\n","  Total_EPOCHS = total_epochs\n","  already_find_best_val_acc_count = 0\n","  current_learning_rate = INITIAL_LR\n","\n","  # the folder where the trained model is saved\n","  CHECKPOINT_FOLDER = \"./saved_model\"\n","  # start the training/validation process\n","  # the process should take about 5 minutes on a GTX 1070-Ti\n","  # if the code is written efficiently.\n","  best_val_acc = 0\n","  best_val_epoch = 0\n","  epoch_history = []\n","  train_accuracy_history = []\n","  validation_accuracy_history = []\n","\n","  print(\"==> Training starts!\")\n","  print(\"=\"*50)\n","  for i in range(0, Total_EPOCHS):\n","      if i in change_lr_epochs:\n","            current_learning_rate = current_learning_rate * DECAY\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] = current_learning_rate\n","            print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n","        \n","      net.train()\n","      #######################\n","      epoch_history.append(i)\n","      # this help you compute the training accuracy\n","      total_examples = 0\n","      correct_examples = 0\n","      train_loss = 0 # track training loss if you want\n","      # Train the model for 1 epoch.\n","      for batch_idx,(data,labels) in enumerate(train_loader):\n","          data = data.to(device); labels = labels.to(device)\n","          adv_data = attacks.PGD_attack(net, device, data, labels, ATK_EPS, ATK_ALPHA, ATK_ITERS, RND_START, min_val, max_val)\n","          # Forward pass\n","          outputs = net(adv_data)\n","          detached_targets = labels.detach()\n","          Manual_output = torch.full((detached_targets.size()[0], n_classes), (1 - a) / (n_classes - 1))\n","          for i in range(detached_targets.size()[0]):\n","            Manual_output[i][detached_targets[i]] = a\n","          Manual_output = Manual_output.to(device)\n","          soften_outputs = F.log_softmax(torch.div(outputs, temperature),dim =1)\n","          Manual_output = F.softmax(torch.div(Manual_output, temperature),dim =1)\n","          net.zero_grad()\n","          optimizer.zero_grad()\n","          # Compute loss, gradients, and update params\n","          loss = (1-alpha) * criterion(outputs, labels) + alpha * DKL(soften_outputs, Manual_output)\n","          loss.backward()\n","          optimizer.step()\n","          # Update stats\n","          _,preds = outputs.max(1)\n","          correct_examples += preds.eq(labels).sum().item()\n","          train_loss += loss.item()\n","          total_examples += labels.size(0)\n","          ####################################\n","                  \n","      avg_loss = train_loss / len(train_loader)\n","      avg_acc = correct_examples / total_examples\n","      train_accuracy_history.append(avg_acc)\n","      print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n","      # switch to eval mode\n","      net.eval()\n","      # this help you compute the validation accuracy\n","      total_examples = 0\n","      correct_examples = 0\n","      val_loss = 0 # again, track the validation loss if you want\n","      # disable gradient during validation, which can save GPU memory\n","      # with torch.no_grad():\n","      for batch_idx, (inputs, targets) in enumerate(test_loader):\n","          ####################################\n","          inputs = inputs.to(device)\n","          targets = targets.to(device)\n","          # compute the output and loss\n","          outputs = net(inputs)\n","          # outputs = net(adv_data)\n","          loss = criterion(outputs, targets)\n","          val_loss += loss\n","          # count the number of correctly predicted samples in the current batch\n","          _, pre_out = torch.max(outputs.data, 1)\n","          correct_examples = correct_examples + torch.sum(pre_out == targets)\n","          total_examples = total_examples + targets.shape[0]\n","          ####################################\n","\n","      avg_loss = val_loss / len(test_loader)\n","      avg_acc = correct_examples / total_examples\n","      validation_accuracy_history.append(avg_acc)\n","      print(\"Validation loss: %.4f, Validation accuracy: %.4f, Best Validation accuracy: %.4f best_val_epoch: %d\" % (avg_loss, avg_acc, best_val_acc, best_val_epoch))\n","      # save the model checkpoint\n","      model_checkpoint = \"PGD_Adv_Teacher_Free_KD_ResNet18.pth\"\n","      if avg_acc > best_val_acc:\n","          best_val_acc = avg_acc\n","          best_val_epoch = i\n","          already_find_best_val_acc_count = 0\n","          if not os.path.exists(CHECKPOINT_FOLDER):\n","              os.makedirs(CHECKPOINT_FOLDER)\n","          print(\"Saving ...\")\n","          state = {'state_dict': net.state_dict(),\n","                  'epoch': i,\n","                  'lr': current_learning_rate}\n","          torch.save(state, os.path.join(CHECKPOINT_FOLDER, model_checkpoint))\n","      else:\n","        already_find_best_val_acc_count = already_find_best_val_acc_count + 1\n","          \n","      print('')\n","\n","  print(\"=\"*50)\n","  print(f\"==> Optimization finished! Best validation accuracy: {best_val_acc:.4f}\")\n","  print(\"best_val_epoch: \" + str(best_val_epoch))\n"],"metadata":{"id":"p3bZuPkvhQ_U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Autoattacks"],"metadata":{"id":"1JPL0H0p-gqf"}},{"cell_type":"code","source":["!pip install torchattacks"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fGZruWIwJGSS","outputId":"aea4bd0f-2d85-4b74-cb54-7b285caab274"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchattacks\n","  Downloading torchattacks-3.3.0-py3-none-any.whl (155 kB)\n","\u001b[K     |████████████████████████████████| 155 kB 4.7 MB/s \n","\u001b[?25hInstalling collected packages: torchattacks\n","Successfully installed torchattacks-3.3.0\n"]}]},{"cell_type":"code","source":["import torchattacks"],"metadata":{"id":"PUwesR2BI8QA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def attack_with_auto(whitebox_mdl, eps):\n","  whitebox = ResNet(BasicBlock, [2, 2, 2, 2])\n","  state_dict = torch.load(whitebox_mdl) # change the path to your own checkpoint file\n","  whitebox.load_state_dict(state_dict['state_dict'])\n","  whitebox = whitebox.to(device)\n","  whitebox.eval(); \n","\n","  test_acc,_ = test_model(whitebox,test_loader,device)\n","  print(\"Initial Accuracy of Whitebox Model: \",test_acc)\n","  wb_accuracies = dict()\n","  for type_attack in ['Autoattack']:\n","    print(type_attack)\n","    \n","    ATK_EPS = eps\n","    ATK_ITERS = 10\n","    ATK_ALPHA = 1.85*ATK_EPS / ATK_ITERS\n","    RND_START = True\n","\n","    whitebox_correct = 0.\n","    blackbox_correct = 0.\n","    running_total = 0.\n","    for batch_idx,(data,labels) in enumerate(test_loader):\n","      print('Batch ' + str(batch_idx))\n","      data = data.to(device) \n","      labels = labels.to(device)\n","      # Adversarial attack \n","      attack = torchattacks.AutoAttack(whitebox, norm='Linf', eps=eps, version='standard', n_classes=10, seed=None, verbose=False)\n","      adv_data = attack(data, labels)\n","      \n","      # Compute accuracy on perturbed data\n","      with torch.no_grad():\n","          # Stat keeping - whitebox\n","          whitebox_outputs = whitebox(adv_data)\n","          _,whitebox_preds = whitebox_outputs.max(1)\n","          whitebox_correct += whitebox_preds.eq(labels).sum().item()\n","          running_total += labels.size(0)\n","          # Print final \n","    whitebox_acc = whitebox_correct/running_total\n","\n","    print(whitebox_acc)\n"],"metadata":{"id":"AvKE1pS2IYlQ"},"execution_count":null,"outputs":[]}]}