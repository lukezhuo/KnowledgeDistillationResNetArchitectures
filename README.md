# Knowledge Distillation for ResNet Architectures #
## Code for Approaches to Knowledge Distillation ##

Knowledge_Distillation_Code file contains methods that allow for the training of:
* ResNet18 
* ResNet50
* Knowledge Distillation (KD) with ResNet50 Teacher ResNet18 Student
* Reversed Knowledge Distillation (Re-KD) with ResNet18 Teacher ResNet50 Student
* Teacher-Free Self Knowledge Distillation
* Teacher-Free Knowledge Distillation Manually-Designated Regularization
* ResNet18 Adversarially Trained (AT ResNet18)
* Teacher-Free Self Knowledge Distillation Adversarially Trained (AT Tf KD-self)
* Teacher-Free Manually-Designated Regularization Adversarially Trained (AT Tf KD-reg)

Adversarial Training uses PGD, found in attacks.py
